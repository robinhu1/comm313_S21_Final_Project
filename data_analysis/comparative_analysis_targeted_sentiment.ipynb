{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to attempt a sentiment analysis and gather the data together into a df. I will likely use VAD because of the three different scores I can look at for each source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import seaborn as sn\n",
    "from collections import Counter\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.cluster import hierarchical,KMeans, linkage_tree\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I need to load all my files relevant here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_corp = json.load(open('../data/text/china_daily/cd_corpus_index.json'))\n",
    "nyt_corp = json.load(open('../data/text/nyt/nyt_corpus_index.json'))\n",
    "dt_corp = json.load(open('../data/text/daily_telegraph/dt_corpus_index.json'))\n",
    "g_corp = json.load(open('../data/text/guardian/guardian_corpus_index.json'))\n",
    "ht_corp = json.load(open('../data/text/hindustan_times/ht_corpus_index.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in cd_corp[:32]:\n",
    "    filename = article['Filename']\n",
    "    text = open('../data/text/china_daily/{}'.format(filename)).read()\n",
    "    article['text'] = text\n",
    "for article in cd_corp[32:]:\n",
    "    filename = article['Filename']\n",
    "    text = open('../data/text/china_daily/{}'.format(filename)).read()\n",
    "    body_text_start = text.index('Body')+4\n",
    "    body_text_end = text.find('Load-Date:')\n",
    "    body_text=text[body_text_start:body_text_end].strip()\n",
    "    article['text'] = body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in nyt_corp:\n",
    "    filename = article['Filename']\n",
    "    text = open('../data/text/nyt/{}'.format(filename)).read()\n",
    "    article['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in dt_corp:\n",
    "    if article['Filename'].startswith('wish-magazine'):\n",
    "        dt_corp.remove(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in dt_corp:\n",
    "    filename = article['Filename']\n",
    "    text = open('../data/text/daily_telegraph/{}'.format(filename)).read()\n",
    "    article['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in g_corp:\n",
    "    filename = article['Filename']\n",
    "    text = open('../data/text/guardian/{}'.format(filename)).read()\n",
    "    article['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in ht_corp:\n",
    "    filename = article['Filename']\n",
    "    text = open('../data/text/hindustan_times/{}'.format(filename)).read()\n",
    "    article['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_to_remove = '!,.()[]|\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing a smaller unit of analysis that consists of articles that contain at leaast one of the origin terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_terms = ['laboratory','lab','bioweapon','market','military','cold-chain','conspiracy','army','detrick', 'transparency','origins','wuhan','theory','imported']\n",
    "origin_txt_cd=[]\n",
    "for word in origin_terms:\n",
    "    for article in cd_corp:\n",
    "        if article in origin_txt_cd:\n",
    "            continue\n",
    "        elif article['text'].count(word)>0:\n",
    "            origin_txt_cd.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_txt_nyt=[]\n",
    "for word in origin_terms:\n",
    "    for article in nyt_corp:\n",
    "        if article in origin_txt_nyt:\n",
    "            continue\n",
    "        elif article['text'].count(word)>0:\n",
    "            origin_txt_nyt.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_txt_dt=[]\n",
    "for word in origin_terms:\n",
    "    for article in dt_corp:\n",
    "        if article in origin_txt_dt:\n",
    "            continue\n",
    "        elif article['text'].count(word)>0:\n",
    "            origin_txt_dt.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_txt_g=[]\n",
    "for word in origin_terms:\n",
    "    for article in g_corp:\n",
    "        if article in origin_txt_g:\n",
    "            continue\n",
    "        elif article['text'].count(word)>0:\n",
    "            origin_txt_g.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_txt_ht=[]\n",
    "for word in origin_terms:\n",
    "    for article in ht_corp:\n",
    "        if article in origin_txt_ht:\n",
    "            continue\n",
    "        elif article['text'].count(word)>0:\n",
    "            origin_txt_ht.append(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for some VAD analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRC_VAD_lexicon = open('NRC-VAD-Lexicon.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRC_VAD = {}\n",
    "for line in NRC_VAD_lexicon[1:]:  \n",
    "    word, V,A,D = line.strip().split('\\t')\n",
    "    NRC_VAD[word] = {'V': float(V), \n",
    "                     'A': float(A),\n",
    "                     'D': float(D)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(article):\n",
    "    toks= tokenize(article['text'], lowercase=True, strip_chars=characters_to_remove)\n",
    "    article['tokens'] = toks\n",
    "    \n",
    "    article['Valence']=0\n",
    "    article['Dominance']=0\n",
    "    article['Arousal']=0\n",
    "    \n",
    "    article['VAD_toks']=[]\n",
    "    \n",
    "    for a in toks:\n",
    "        if a.lower() in NRC_VAD.keys():\n",
    "            scores = NRC_VAD[a.lower()]\n",
    "            scores['tok']=a\n",
    "            \n",
    "            article['Valence']+=scores['V']\n",
    "            article['Arousal']+=scores['A']\n",
    "            article['Dominance']+=scores['D']\n",
    "            \n",
    "            article['VAD_toks'].append(scores)\n",
    "    \n",
    "    \n",
    "    for dimension in ('Valence','Arousal','Dominance'):\n",
    "        if len(article['VAD_toks'])>0:\n",
    "            article[dimension] /= len(article['VAD_toks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am just generating V, A, and D scores for each text in each source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_cd = []\n",
    "a_cd = []\n",
    "d_cd = []\n",
    "for article in origin_txt_cd:\n",
    "    process_article(article)\n",
    "    v_cd.append(article.get('Valence',''))\n",
    "    a_cd.append(article.get('Arousal',''))\n",
    "    d_cd.append(article.get('Dominance',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_nyt = []\n",
    "a_nyt = []\n",
    "d_nyt = []\n",
    "for article in origin_txt_nyt:\n",
    "    process_article(article)\n",
    "    v_nyt.append(article.get('Valence',''))\n",
    "    a_nyt.append(article.get('Arousal',''))\n",
    "    d_nyt.append(article.get('Dominance',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dt = []\n",
    "a_dt = []\n",
    "d_dt = []\n",
    "for article in origin_txt_dt:\n",
    "    process_article(article)\n",
    "    v_dt.append(article.get('Valence',''))\n",
    "    a_dt.append(article.get('Arousal',''))\n",
    "    d_dt.append(article.get('Dominance',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_g = []\n",
    "a_g = []\n",
    "d_g = []\n",
    "for article in origin_txt_g:\n",
    "    fpath = os.path.join('..','data','text', 'guardian', article['Filename'])\n",
    "    text = open(fpath).read()\n",
    "    tokens = tokenize(text, lowercase=True, strip_chars=characters_to_remove)\n",
    "    article['tokens']= tokens\n",
    "    article['Valence']=0\n",
    "    article['Dominance']=0\n",
    "    article['Arousal']=0\n",
    "    \n",
    "    article['VAD_toks']=[]\n",
    "    \n",
    "    for a in tokens:\n",
    "        if a.lower() in NRC_VAD.keys():\n",
    "            scores = NRC_VAD[a.lower()]\n",
    "            scores['tok']=a\n",
    "            \n",
    "            article['Valence']+=scores['V']\n",
    "            article['Arousal']+=scores['A']\n",
    "            article['Dominance']+=scores['D']\n",
    "            \n",
    "            article['VAD_toks'].append(scores)\n",
    "    \n",
    "    \n",
    "    for dimension in ('Valence','Arousal','Dominance'):\n",
    "        if len(article['VAD_toks'])>0:\n",
    "            article[dimension] /= len(article['VAD_toks'])\n",
    "            \n",
    "    v_g.append(article.get('Valence',''))\n",
    "    a_g.append(article.get('Arousal',''))\n",
    "    d_g.append(article.get('Dominance',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_ht = []\n",
    "a_ht = []\n",
    "d_ht = []\n",
    "for article in origin_txt_ht:\n",
    "    process_article(article)\n",
    "    v_ht.append(article.get('Valence',''))\n",
    "    a_ht.append(article.get('Arousal',''))\n",
    "    d_ht.append(article.get('Dominance',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['Valence','Arousal','Dominance']\n",
    "vad_data = {'China Daily':[sum(v_cd)/len(v_cd),sum(a_cd)/len(a_cd),sum(d_cd)/len(d_cd)], \n",
    "            'NY Times': [sum(v_nyt)/len(v_nyt),sum(a_nyt)/len(a_nyt),sum(d_nyt)/len(d_nyt)],\n",
    "            'Daily Telegraph': [sum(v_dt)/len(v_dt),sum(a_dt)/len(a_dt),sum(d_dt)/len(d_dt)],\n",
    "            'The Guardian': [sum(v_g)/len(v_g),sum(a_g)/len(a_g),sum(d_g)/len(d_g)],\n",
    "            'Hindustan Times':[sum(v_ht)/len(v_ht),sum(a_ht)/len(a_ht),sum(d_ht)/len(d_ht)]}\n",
    "\n",
    "vad_df = pd.DataFrame(data=vad_data, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_df.to_csv('../final_data_story/visualizations/vad_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I exported to a csv file that I can easily load in my final data story. It's interesting to see that there aren't that many significant differences although I expected there to be more. This is worth pondering in the markdown explanations of my data story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
